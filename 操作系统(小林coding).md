# 操作系统(小林coding)

## 一、硬件结构

### 1.1 CPU是如何执行程序的？

**冯诺依曼模型**

![image-20230516115743671](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516115743671.png)

定义了计算机基本结构

1. 中央处理器（CPU）
2. 内存
3. 输入设备
4. 输出设备
5. 总线



**内存**

程序和数据都是存储在内存的，存储的区域是线性的。

数据存储的单位是一个二进制位（bit），即0或1。最小的存储单位是字节（byte），1byte=8bit

内存的地址从0开始编号，然后递增排列，结构类似数组，因此内存读写任何一个数据的速度都是一样的



**中央处理器**（CPU）

32位和64位CPU最主要的区别在于一次能计算多少字节数据：

- 32位CPU一次可以计算4个字节
- 64位PU一次可以计算8个字节

32位和64位通常称为CPU的位宽

CPU这样的设计的原因是为了能计算更大的值，如果是8位CPU，一次只能计算1个字节，即0~255的范围，这样就无法一完成计算10000*500，于是为了能够一次计算大数的运算，CPU需要支持多个byte一起计算，因此CPU位宽越大，可以计算的数值也就越大。



CPU内部还有一些组件，常见的有寄存器、控制单元和逻辑运算单元等。

- 控制单元：负责控制CPU工作
- 逻辑运算单元：负责计算
- 寄存器：有多种类型，每种寄存器的功能不一样。主要储存计算时的数据，寄存器就在CPU里，且紧挨着控制单元和逻辑运算单元，计算速度更快。
  1. 通用寄存器：用来存放需要进行运算的数据
  2. 程序计数器：用来存储CPU要执行的下一条指令**所在的内存地址**，此时指令还在内存中
  3. 指令寄存器：用来存放程序计数器指向的指令，即指令本身，指令被执行完之前都存储在这里



***为什么有程序计数器存放下一条指令的内存地址，还需要指令寄存器呢？***

程序计数器和指令寄存器都是计算机中重要的寄存器之一，但是它们的作用是不同的。程序计数器用于存放下一条指令在内存中的地址，而指令寄存器则是用于存放当前正在执行的指令。

当计算机需要执行指令时，程序计数器会把下一条指令的地址取出来，然后将其存放到指令寄存器中，执行该指令后指令寄存器会自动移动到下一条指令的内存地址。这样计算机就能顺序执行程序中的指令了。

另外，指令寄存器还可以用于存放预取出的指令，这样可以提高指令的执行效率。当执行完当前指令后，指令寄存器可以直接取出预取出的指令执行，而不需要重新从内存中取出下一条指令的地址，从而节省了时间。



**总线**

总线用于CPU和内存以及其他设备之间的通信，可分为3种

- 地址总线：用于指定CPU将要操作的内存地址
- 数据总线：用于读写内存的数据
- 控制总线：用于发送和接收信号，比如中断、设备复位等信号，

当CPU要读写内存数据时，一般通过两个总线：

- 

- 首先通过地址总线指定内存地址
- 再通过数据总线来传输数据



**输入、输出设备**

输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键是需要和CPU进行交互的，此时就需要用到控制总线了。



**线路位宽与CPU位宽**

数据通过线路电压的变化来传输的，低电压表示0，高电压表示1

一位一位的传输方式称为串行，下一个bit必须等待上一个bit传输完成后才能进行传输。如果想一次传输多位数据，只需要增加线路即可，此时数据可以并行传输。

为了避免低效率的串行传输的方式，线路的位宽最好一次就能访问到所有的内存地址。CPU要想操作内存地址就需要总线，如果总线只有一条，那每次只能表示0或1这两种情况，所以CPU一次只能操作两个内存地址了，如果CPU要想操作4G的内存，就需要32条地址总线，因为log2(4G)=32

CPU的位宽最好不要小于线路位宽，32位CPU一次最多只能操作32位宽的地址总线和数据总线

如果计算的数额不超过2^32的情况下，32位和64位CPU之间的处理速度没什么区别

32位CPU最大只能操作4GB内存，就算有8GB内存条也没用，因为寻址不到。而64位CPU的理论最大寻址空间2^64



**程序执行的基本过程**

程序实际上是一条一条指令，程序的运行过程实际上就是一条一条执行指令，负责执行指令的就是CPU了

![](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516172737104.png)

1. CPU获取程序计数器的值，该值是指令的内存地址，然后CPU的控制单元通过操作地址总线指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过数据总线将指令数据传给CPU，CPU接收到内存传来的地址后，将这个指令数据存入到指令寄存器
2. CPU分析指令寄存器中的指令，确定之类的类型和参数，如果是计算类型的指令，就把指令交给逻辑运算单元运算；如果是存储类型的指令，则交给控制单元执行
3. CPU执行完指令后，程序计数器的值自增，表示指向下一条指令。自增的大小由CPU的位宽决定，比如32位的CPU，指令是4个字节，需要4个内存地址存放，因此程序计数器的值会增加4

总结：一个从程序执行的时候，CPU会根据程序计数器的内存地址，从内存里把需要执行的指令读取到指令寄存器里面指向。然后根据指令长度自增，即自增到下一个指令内存地址，然后又开始循环反复



**a=1+2的执行具体过程**

CPU不认识a=1+2这个字符串，这个字符串只是方便我们程序员认识。要想将这段程序跑起来，还需要把整个程序编译成汇编代码。

针对汇编代码，还需要把汇编代码翻译成机器码，即由0和1组成的机器语言。

程序编译过程中，编译器通过分析代码，发现1和2是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是**数据段**，注意，数据和指令是分开区域存放的，存放指令区域的地方叫做**正文段**，如下图

![image-20230516175213486](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516175213486.png)

![image-20230516175315117](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516175315117.png)

注意，以上是编译器干的事

由于是在32位CPU执行的，因此一条指令是占32位大小，所以每条指令间隔4个字节

而数据的大小是根据程序中指定的变量类型，比如int类型的数据占4个字节，char类型的数据占1个字节



**指令**

上图中的指令内容实际上是一串二进制数字的机器码，简易的汇编代码实际上只是方便我们理解

不同的CPU有不同的指令集，也就对应着不同的汇编语言和不同的机器码

![image-20230516175730079](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516175730079.png)

- R指令：用在算数和逻辑操作，里面有读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的位移量，而最后的功能码则是再前面的操作码不够时，扩展操作码；来表示对应的具体指令
- I指令：用在数据传输、条件分支等。这个类型的指令，就没有了位移量和操作码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或一个常数
- J指令：用在跳转，高6位之外的26位都是一个跳转的目标地址

![image-20230516180206781](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516180206781.png)

编译器在编译程序时会构造指令，这个过程叫做指令的编码。CPU执行程序时，就会解析指令，这个过程叫做指令的解码。

现代大多数CPU都使用流水线的方式来执行指令，所谓的流水线实际上就是把一个任务拆分成多个小任务，于是一条指令通常分为4个阶段，称为4级流水线，如下图：
![image-20230516180526634](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516180526634.png)

1. Fetch：CPU通过程序计数器读取对应内存地址的指令
2. Decode：CPU对指令进行解码
3. Execute：CPU执行指令
4. Store：CPU将计算结果存回寄存器或者将寄存器的值存入内存

这4个阶段，称为指令周期（Instruction Cycle），CPU的工作就是一个周期接着一个周期，周而复始

实际上，不同阶段其实是由计算机中不同组件完成的

![image-20230516180919407](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516180919407.png)



**指令的类型**

![image-20230516181111705](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516181111705.png)



**指令的执行速度**

![image-20230516181343630](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516181343630.png)

假设CPU的主频是2.4GHz，意味着1秒钟会产生2.4G次的脉冲信号，则时钟周期时间是(1/2.4G)s

提升CPU执行效率可以从两方面入手：

1. 降低时钟周期时间，即提高CPU主频，但今非昔比，摩尔定律早已失效，CPU主频很难再做到翻倍成长
2. 降低CPU时钟周期数

CPU时钟周期数可以拆解成 指令数*每条指令的平均时钟周期数（Cycles Per Instruction，简称CPI）

![image-20230516181942446](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516181942446.png)

![image-20230516182130394](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516182130394.png)



**总结**

![image-20230516183411326](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230516183411326.png)



### 1.2 存储器金字塔

内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，而硬盘不会，因为硬盘是持久化存储设备，同时也是一个I/O设备

CPU内部也有存储数据的组件，比如寄存器、CPU的L1/L2/L3 Cache，只不过存储的数据非常小，但又因为接近CPU核心，所以访问速度很快，比硬盘快好几个数量级



**存储器的层次结构**

1. CPU内部

   - CPU：大脑

   - CPU中的寄存器：大脑正在思考的东西，处理速度最快，但是能存储的数据也是最少的

   - CPU Cache：大脑中的记忆

     ——L1 Cache：数据存储和指令存储，L1距离CPU最近，所以它比L2、L3的读写速度都更快、存储空间更小

     ——L2/L3 Cache：大脑中的长期记忆

     寄存器和CPU Cache都在CPU内部，跟CPU距离近，因此读写速度都快，但是内存小

2. CPU外部

   - 内存：书桌上的书，虽然一伸手就可以拿到，但是读写速度远慢于寄存器
   - 硬盘：图书馆书架上的书，能存储的数据十分大，但是读写速度比内存差好几个数量级

   ![image-20230518230811525](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230518230811525.png)

从图书馆书架取书，把书放到桌子上，再阅读书，大脑记忆知识点，然后经过大脑思考。这一过程相当于从硬盘加载到内存，再从内存加载到CPU的寄存器和Cache中，然后再通过CPU进行处理和计算

对于存储器，速度越快 -> 能耗越高 -> 材料成本越贵，所以速度快的存储器容量都比较小

存储器可以分为以下几个级别：

- 寄存器
- CPU Cache：
  1. L1-Cache
  2. L2-Cache
  3. L3-Cache
- 内存
- SSD/HDD硬盘



**寄存器**

最靠近CPU控制单元和逻辑计算单元的存储器

- 32位CPU中大多数寄存器可以存储4个字节
- 64位CPU中大多数寄存器可以存储8个字节

寄存器的访问速度非常快，一般要求在半个CPU时钟周期内完成读写，CPU时钟周期跟CPU主频息息相关，比如2GHz主频的CPU的时钟周期就是1/2G，也就是0.5ns

CPU处理一条指令的时候，除了读写寄存器，还需要解码指令、控制指令执行和计算。如果寄存器的速度太慢，则会拉长指令的处理周期，从而给用户感觉电脑"很慢"



**CPU Cache**

CPU Cache用的是一种叫SRAM(Static Random-Access Memory，静态随机存储器)的芯片

SRAM之所以叫静态存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了

在SRAM中，一个bit的数据，通常需要6晶体管，所以SRAM存储密度不高，同样的物理空间下，能存储的数据有限，不过因为SRAM电路简单，所以访问速度非常快

![image-20230519103257378](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230519103257378.png)



**L1高速缓存**

L1高速缓存的访问速度几乎和寄存器一样快，通常只需要2~4个时钟周期，大小在几十KB到几百KB不等

每个CPU核心都有一块属于自己的L1高速缓存，指令和数据在L1是分开存放的，所以L1高速缓存通常分成指令缓存和数据缓存



**L2高速缓存**

L2高速缓存同样存在于每个CPU核心，但离CPU核心的位置比L1更远，但大小比L1大，CPU型号不同大小也不同，通常在几百KB到几MB不等，访问速度更慢，在10~20个时钟周期



**L3高速缓存**

L3高速缓存通常是多个CPU核心共用，位置比L2高速缓存距离CPU核心更远，但大小更大，通常在几MB到几十MB不等，访问速度在20~60个时钟周期



**内存**

内存使用DRAM(Dynamic Random Access Memory，动态随机存取存储器)的芯片

相比SRAM，DRAM的密度更高，功耗更低，有更大的容量，而且造价比SRAM芯片便宜很多

DRAM存储一个bit数据，只需要一个晶体管和一个电容，但是因为数据会被存储在电容里，电容会不断漏电，所以需要定时刷新电容，才能保证数据不会丢失，这就是DRAM被称为动态存储器的原因，只有不断刷新，数据才不会丢失，才能够存储起来

DRAM的数据访问电路和刷新电路都比SRAM更复杂，所以访问速度会更慢，内存速度大概在200~300个时钟周期



**SSD/HDD硬盘**

SSD（Solid-state disk），固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还在，而内存、寄存器、高速缓存断电后数据都会丢失。内存读写的速度比SSD快10~1000倍

还有一种传统硬盘，即机械硬盘（Hard Disk Drive，HDD），通过物理读写的方式来访问数据，因此访问速度非常慢，比内存慢10w倍左右

由于SSD价格接近机械硬盘，因此机械硬盘已经逐渐被SSD替代了



**存储器的层次关系**

CPU并不会直接和每一种存储器设备直接打交道，而是每一种存储器设备只和它相邻的存储器设备打交道

![image-20230519205756455](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230519205756455.png)

当CPU需要访问内存中的某个数据时，如果寄存器有这个数据，CPU直接从寄存器取数据即可，如果寄存器没有这个数据，CPU就会查询L1高速缓存，如果L1没有，则查询L2高速缓存，L2还是没有就查询L3高速缓存，L3依然没有的话，才去内存中取数据。（顿时理解了为什么传的是地址而不是其它了，因为地址是唯一的，不会出现歧义，而值可能有相等的两个值出现的情况）所以，存储层次结构也形成了缓存的体系



**存储器之间的实际价格和性能差距**

![image-20230519235919850](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230519235919850.png)



**总结**

不同的存储器之间性能差距很大，构造存储器分级很有意义，分级的目的是要构造缓存体系



### 1.3 如何写出让CPU跑得更快的代码？

**CPU Cache有多快？**

CPU和内存的访问速度的增长速率不匹配，导致CPU与内存的访问速度相差200~300多倍，意味着内存已经不满足CPU的需求了。为了弥补CPU与内存二者之间的性能差异，就在CPU内部引入了CPU Cache，也称高速缓存

CPU Cache通常分为大小不等的三级缓存，分别是L1 Cache、L2 Cache和L3 Cache

L3 Cache比L1 Cache和L2 Cache大很多，这是因为L1 Cache和L2 Cache都是每个CPU核心独有的，而L3 Cache是多个CPU核心共享的

![image-20230520083808622](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230520083808622.png)



**CPU Cache的数据结构和读取过程是什么样的？**

CPU Cache的数据是从内存中读取过来的，以一小块一小块读取数据的，而不是按照单个数据元素读取数据。在CPU Cache中，这样一小块一小块的数据，称为Cache Line（缓存块）

比如，有一个int array[100]的数组，载入array[0]时，由于数组元素大小在内存只占4字节，不足64字节，CPU就会顺序加载数组元素到array[15]，意味着array[0]~array[15]数组元素都会被缓存在CPU Cache中，因此当下次访问这些数组元素时，会直接从CPU Cache读取，而不需要再从内存中读取，大大提高了CPU读取数据的性能。

事实上，CPU读取数据时，无论数据是否存放到Cache中，CPU都是先访问Cache，只有当Cache中找不到数据时，才回去访问内存，并把内存中的数据读入到Cache中，CPU再从CPU Cache读取数据

![image-20230522114337651](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230522114337651.png)

这样的访问机制，和使用 内存作为硬盘的缓存 的逻辑是一样的，如果内存有缓存的数据，则直接返回，否则要访问龟速般的硬盘

CPU访问内存数据时，是一小块一小块数据读取的，具体大小取决于coherency_line_size的值，一般是64字节。在内存中，这块数据称为内存块（Block），读取时我们需要拿到数据所在内存块的地址。

**直接映射Cache（Direct Mapped Cache）**

把内存块的地址始终 映射 在一个CPU Line（缓存块）的地址，至于映射关系实现方式，则是使用 取模运算，取模运算的结果就是内存块地址对应的CPU Line（缓存块）的地址

![image-20230522114953280](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230522114953280.png)

为了区别不同的内存块，在对应的CPU Line中会存储一个组标记（Tag），记录当前CPU Line中存储的数据对应的内存块，可以使用组标记来区分不同组但映射到同一块CPU Line的内存块。

除了组标记信息外，CPU Line还有两个信息：

1. 从内存加载过来的实际存放数据（Data）
2. 有效位（Valid bit），它是用标记对应的CPU Line中的数据是否是有效的，如果有效位是0，无论CPU Line中是否有数据，CPU都会直接访问内存，重新加载数据。

CPU在从CPU Cache读取数据时，并不是读取CPU Line中的整个数据块，而是读取CPU所需要的一个数据片段（Block），这样的数据统称为一个字（Word），即一个特。在对应的CPU Line中数据块中找到所需的字需要一个偏移量（Offset）

因此，一个内存的访问地址，包括组标记、CPU Line索引、偏移量，对于CPU Cache里的数据结构，则是由索引+有效位+组标记+数据块组成

![image-20230522233014701](https://md-jomo.oss-cn-guangzhou.aliyuncs.com/IMG/image-20230522233014701.png)

如果内存中的数据已经在CPU Cache中，CPU访问一个内存地址的时候，会经历以下4个步骤：

1. 根据内存地址中索引信息，计算在CPU Cache中的索引，也就是找出对应的CPU Line的地址
2. 找到对应CPU Line后，判断CPU Line中的有效位，确认CPU LIne中数据是否有效，如果无效，CPU直接访问内存，并重新加载数据，如果数据有效，则往下执行
3. 对比内存地址中组标记和CPU Line中的组标记，确认CPU Line中的数据是我们要访问的内存数据，如果不是的话，CPU就会直接访问内存，并重新加载数据，如果是的话，则往下执行
4. 根据内存地址中偏移量信息，从CPU Line的数据块中，读取对应的字

其他通过内存地址找到CPU Cache中数据的策略：

- 全相连Cache（Fully Associative Cache）
- 组相连Cache（Set Associative Cache）



**如何写出让CPU跑得更快的代码？**

CPU访问内存的速度比访问CPU Cache的速度慢很多。所以如果 CPU  所要操作的数据在 CPU Cache 中的话，这样将会带来很⼤的性能提升。访问的数据在 CPU  Cache 中的话，意味着缓存命中，缓存命中率越⾼的话，代码的性能就会越好，CPU 也就跑 的越快。 于是，「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命 中率⾼的代码？」。

L1 Cache通常分为数据缓存和指令缓存，因为CPu会分别处理数据和指令，比如 1+1=2 这个运算，+ 就是指令，会被放在「指令缓存」中，而输⼊数 字 1 则会被放在「数据缓存」里。

因此，要分开看 数据缓存 和 指令缓存 的缓存命中率

eg.

一行一行遍历二维数组的速度快于一列一列遍历二维数组，是因为二维数组所占用的内存是连续的，且是按行顺序来保存的，局部性原理。

CPU具体会一次从内存中加载多少元素到CPU Cache和CPU Cache Line有关，表示CPU Cache一次性能加载数据的大小（Word）。

因此，遇到遍历数组的问题，按照内存布局顺序访问访问，可以有效利用CPU Cache带来的好处，提高代码性能



**如何提高指令缓存的命中率？**

eg.

```cpp
//有一个元素为0到99之间随机数字组成的一维数组
int array[N];
for(int i=0;i<N;i++){
    array[i]=rand()%100;
}

//接下来，对这个数组做两个操作
//操作一：数组遍历
for(i=0;i<N;i++){
    if(array[i]<50){
        array[i]=0;
    }
}

//操作二：排序
sort(array,array+N);
```

问题：先遍历再排序快，还是先排序再遍历快

先了解CPU的分支预测期，对于if条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是if还是else中的指令。那么，如果分支预测可以预测到接下来要执行if里的指令还是else指令的话，就可以 提前 把这些指令放在缓存中，这样CPU可以直接从Cache读取到指令，于是执行速度就会很快

当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是有序时，分支预测会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。

因此先排序再遍历速度会快很多，因为排序后，数字是从小到大的，前几次循环命中if<50的次数会比较多，于是分支预测就会缓存if里的array[i]=0指令到Cache中，后序CPU执行该指令就只需要从Cache读取即可

如果你肯定代码中if中表达式判断为true的概率比较高，可以使用显示分支预测工具，比如在C/C++语言中编译器提供了likely和unlikely这两种宏，可以用likely宏把if里的表达式包裹起来，反之用unlikely宏

```cpp
#define likely(x)__builtin_expect(!!(x),1)
#define unlikely(x)__builtin_expect(!!(x),0)

if(likely(a==1)){
    //do something...
}else{
    //do something...
}
```

实际上，CPU自身的动态分支预测已经比较准了，只用当非常确信CPU预测的不准，且能够知道实际的概率情况时，才建议使用这两种宏。



**如何提升多核CPU的缓存命中率？**

在单核CPU，虽然只能执行一个进程，但是操作系统给每个进程分配了一个时间片，时间片用完了，就调度下一个进程，于是各个进程就按时间片交替地占用CPU，从宏观上看起来各个进程同时在执行

而现代CPU都是多核心的，进程可能在不同CPU核心来回切换执行，这对CPU Cache不是有利的，虽然L3 Cache事多核心之间共享的，但是L1和L2 Cache是每个核心独有的，如果一个进程在不同核心来回切换，各个核心呃缓存命中率会受到影响，相反如果进程都在同一个核心上执行，其数据的L1和L2 Cache的缓存命中率可以得到有效提高，缓存命中率高意味着CPU可以减少访问内存的频率

当有多个同时执行 计算密集型 的线程，为了防止因切换到不同的核心，而导致缓存命中率下降的问题，可以把线程绑定在某一个CPU核心上，性能得到非常可观的提升。

